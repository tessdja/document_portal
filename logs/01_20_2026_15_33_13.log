{"available_keys": ["GOOGLE_API_KEY", "GROQ_API_KEY"], "timestamp": "2026-01-20T20:33:16.016157Z", "level": "info", "event": "Environment variables validated"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-01-20T20:33:16.017449Z", "level": "info", "event": "Configuration loaded successfully"}
{"temp_base": "data\\multi_doc_chat", "faiss_base": "faiss_index", "session_id": "session_20260120_203316_91b92c20", "temp_path": "data\\multi_doc_chat\\session_20260120_203316_91b92c20", "faiss_path": "faiss_index\\session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:16.017449Z", "level": "info", "event": "DocumentIngestor initialized"}
{"filename": "data\\multi_doc_chat\\market_analysis_report.docx", "saved_as": "data\\multi_doc_chat\\session_20260120_203316_91b92c20\\7c853ba8.docx", "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:16.017449Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\NIPS-2017-attention-is-all-you-need-Paper.pdf", "saved_as": "data\\multi_doc_chat\\session_20260120_203316_91b92c20\\fbe83a4a.pdf", "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:16.100462Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\sample.pdf", "saved_as": "data\\multi_doc_chat\\session_20260120_203316_91b92c20\\61d2fe89.pdf", "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:17.284089Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\state_of_the_union.txt", "saved_as": "data\\multi_doc_chat\\session_20260120_203316_91b92c20\\4d8919dd.txt", "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:25.194184Z", "level": "info", "event": "File saved for ingestion"}
{"total_docs": 90, "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:25.196133Z", "level": "info", "event": "All documents loaded"}
{"total_chunks": 483, "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:25.216903Z", "level": "info", "event": "Documents split into chunks"}
{"timestamp": "2026-01-20T20:33:25.217900Z", "level": "info", "event": "Loading embedding model..."}
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
Loading faiss with AVX512 support.
Could not load library with AVX512 support due to:
ModuleNotFoundError("No module named 'faiss.swigfaiss_avx512'")
Loading faiss with AVX2 support.
Successfully loaded faiss with AVX2 support.
{"path": "faiss_index\\session_20260120_203316_91b92c20", "session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:33.686481Z", "level": "info", "event": "FAISS index saved to disk"}
{"session_id": "session_20260120_203316_91b92c20", "timestamp": "2026-01-20T20:33:33.687490Z", "level": "info", "event": "FAISS retriever created and ready to use"}
{"available_keys": ["GOOGLE_API_KEY", "GROQ_API_KEY"], "timestamp": "2026-01-20T20:33:33.689486Z", "level": "info", "event": "Environment variables validated"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-01-20T20:33:33.690487Z", "level": "info", "event": "Configuration loaded successfully"}
{"timestamp": "2026-01-20T20:33:33.691491Z", "level": "info", "event": "Loading LLM..."}
{"provider": "google", "model": "gemini-2.0-flash", "temperature": 0, "max_tokens": 2048, "timestamp": "2026-01-20T20:33:33.691491Z", "level": "info", "event": "Loading LLM"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-20T20:33:33.726000Z", "level": "info", "event": "LLM loaded successfully"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-20T20:33:33.726000Z", "level": "info", "event": "LCEL graph built successfully"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-20T20:33:33.726000Z", "level": "info", "event": "ConversationalRAG initialized"}
AFC is enabled with max remote calls: 10.
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
AFC is enabled with max remote calls: 10.
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
{"session_id": "test_multi_doc_chat", "user_input": "what is President Zelensky said in their speech in parliament?", "answer_preview": "In his speech to the European Parliament, President Zelenskyy said, \"Light will win over darkness.\"", "timestamp": "2026-01-20T20:33:35.078929Z", "level": "info", "event": "Chain invoked successfully"}

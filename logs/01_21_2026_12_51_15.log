{"available_keys": ["GOOGLE_API_KEY", "GROQ_API_KEY"], "timestamp": "2026-01-21T17:51:18.819869Z", "level": "info", "event": "Environment variables validated"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-01-21T17:51:18.836590Z", "level": "info", "event": "Configuration loaded successfully"}
{"temp_base": "data\\multi_doc_chat", "faiss_base": "faiss_index", "session_id": "session_20260121_175118_785b3fe3", "temp_path": "data\\multi_doc_chat\\session_20260121_175118_785b3fe3", "faiss_path": "faiss_index\\session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:18.836590Z", "level": "info", "event": "DocumentIngestor initialized"}
{"filename": "data\\multi_doc_chat\\market_analysis_report.docx", "saved_as": "data\\multi_doc_chat\\session_20260121_175118_785b3fe3\\9f3bd037.docx", "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:18.836590Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\NIPS-2017-attention-is-all-you-need-Paper.pdf", "saved_as": "data\\multi_doc_chat\\session_20260121_175118_785b3fe3\\4675a4e8.pdf", "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:18.907064Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\sample.pdf", "saved_as": "data\\multi_doc_chat\\session_20260121_175118_785b3fe3\\a6720020.pdf", "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:19.603387Z", "level": "info", "event": "File saved for ingestion"}
{"filename": "data\\multi_doc_chat\\state_of_the_union.txt", "saved_as": "data\\multi_doc_chat\\session_20260121_175118_785b3fe3\\29de9fc2.txt", "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:24.921326Z", "level": "info", "event": "File saved for ingestion"}
{"total_docs": 90, "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:24.936451Z", "level": "info", "event": "All documents loaded"}
{"total_chunks": 483, "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:24.937863Z", "level": "info", "event": "Documents split into chunks"}
{"timestamp": "2026-01-21T17:51:24.937863Z", "level": "info", "event": "Loading embedding model..."}
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
Loading faiss with AVX512 support.
Could not load library with AVX512 support due to:
ModuleNotFoundError("No module named 'faiss.swigfaiss_avx512'")
Loading faiss with AVX2 support.
Successfully loaded faiss with AVX2 support.
{"path": "faiss_index\\session_20260121_175118_785b3fe3", "session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:32.974141Z", "level": "info", "event": "FAISS index saved to disk"}
{"session_id": "session_20260121_175118_785b3fe3", "timestamp": "2026-01-21T17:51:32.974141Z", "level": "info", "event": "FAISS retriever created and ready to use"}
{"available_keys": ["GOOGLE_API_KEY", "GROQ_API_KEY"], "timestamp": "2026-01-21T17:51:32.976147Z", "level": "info", "event": "Environment variables validated"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-01-21T17:51:32.978148Z", "level": "info", "event": "Configuration loaded successfully"}
{"timestamp": "2026-01-21T17:51:32.978148Z", "level": "info", "event": "Loading LLM..."}
{"provider": "google", "model": "gemini-2.0-flash", "temperature": 0, "max_tokens": 2048, "timestamp": "2026-01-21T17:51:32.978148Z", "level": "info", "event": "Loading LLM"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-21T17:51:33.010500Z", "level": "info", "event": "LLM loaded successfully"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-21T17:51:33.011520Z", "level": "info", "event": "LCEL graph built successfully"}
{"session_id": "test_multi_doc_chat", "timestamp": "2026-01-21T17:51:33.012542Z", "level": "info", "event": "ConversationalRAG initialized"}
AFC is enabled with max remote calls: 10.
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents "HTTP/1.1 200 OK"
AFC is enabled with max remote calls: 10.
HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
{"session_id": "test_multi_doc_chat", "user_input": "what is President Zelensky said in their speech in parliament?", "answer_preview": "In his speech to the European Parliament President Zelenskyy said \u201cLight will win over darkness.\u201d", "timestamp": "2026-01-21T17:51:34.269784Z", "level": "info", "event": "Chain invoked successfully"}
